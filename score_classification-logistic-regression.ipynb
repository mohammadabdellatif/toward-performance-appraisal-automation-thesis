{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d939d365-74c1-4c9f-8c28-18c32cebfd3a",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "- Testing 5 vs All, using text tone as percentages\n",
    "    - Logistic Regression was fitted using Recursive Feature Elemination (RFE), and each time the model was trained and tested and it was found that the accuracy maintained 69% until selecting the best 12 features, as after selecting the best 11 features and below the accuracy dropped to 49%\n",
    "    - The best 12 features are: ['wf_resolved', 'wf_open', 'wf_in_progress', 'wf_reopened', 'wf_validation', 'wf_resolved_under_monitoring', 'wf_closed', 'wf_waiting', 'wf_under_review', 'wf_pending_deployment', 'wf_total_time', 'reporter_terms_count'], noticing that no text tone features has effect in this model.\n",
    "    - The Recall for the other label is higher, 87%, but the precision is low which mean means that the model captures more cases in the other labels but there is a high rate for misclassifying category 5 as other\n",
    "- Testing 5 vs All, using text tone as count\n",
    "    - The model after RFE was fitted against 35 features and it gave a slight better performance with 70%\n",
    "    - The selected features are: ['issue_comments_count', 'wf_resolved', 'wf_open', 'wfe_open', 'wf_in_progress', 'wfe_in_progress', 'wf_reopened', 'wf_validation', 'wf_resolved_under_monitoring', 'wf_closed', 'wf_waiting', 'wf_under_review', 'wf_pending_deployment', 'turn', 'wf_total_time', 'processing_steps', 'assignee_utterances_count', 'reporter_utterances_count', 'others_utterances_count', 'assignee_terms_count', 'reporter_terms_count', 'others_terms_count', 'utr_assignee_open_close', 'utr_assignee_inform', 'utr_assignee_assignment_update', 'utr_assignee_status_update', 'utr_reporter_request', 'utr_others_open_close', 'utr_others_user_mention', 'utr_others_update_request', 'issue_type_Ticket', 'issue_priority_High', 'issue_priority_Medium', 'utr_reporter_inform', 'utr_reporter_technical']\n",
    "    - The Recall and precision of the model didn't change\n",
    "    - It was noticed that text tone and related features were part of the 35 selected features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9491380-e3e1-4212-8c19-03ee0a491a07",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Do general imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "284f13a5-123e-4236-9570-e5371ddc3abb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from classifiers.testing import cycle_test,TestType,TestInputs,DatasetFeatures\n",
    "\n",
    "rfe_n_features_without_text_tone = 9 # done\n",
    "rfe_n_features_with_text_tone_as_perentages = 21 # done\n",
    "rfe_n_features_with_text_tone_as_count = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bad3d83-6ffd-436a-934f-60c30c6991d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### The code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36c4c0ed-1301-47c6-a13e-3402dd9a2c1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def pre_process(x,y):\n",
    "    # print(len(x.columns))\n",
    "    num_x = x.select_dtypes(include='number')\n",
    "    x[num_x.columns] = MinMaxScaler().fit_transform(num_x[num_x.columns])\n",
    "    y.loc[y['Q1'] == 5,'Q1'] = 1\n",
    "    return x,y\n",
    "\n",
    "def fit_and_test_statsm(inputs: TestInputs,features_in):\n",
    "    log_reg = sm.Logit(inputs.y_train, sm.add_constant(inputs.x_train[features_in])).fit(maxiter=100,method='lbfgs') \n",
    "    print(log_reg.summary())\n",
    "    predicted_prop = log_reg.model.predict(log_reg.params, exog=sm.add_constant(inputs.x_test[features_in]))   \n",
    "    # print(predicted_prop)\n",
    "    return list(map(round, predicted_prop))    \n",
    "\n",
    "def fit_and_test_sklearn(inputs: TestInputs,features_in):\n",
    "    clf = LogisticRegression().fit(inputs.x_train[features_in],inputs.y_train.iloc[:,0])\n",
    "    predicted = clf.predict(inputs.x_test[features_in])\n",
    "    print(clf.get_params(True))\n",
    "    # for i,f in enumerate(features_in):\n",
    "    #     coef = '{0:.5f}'.format(clf.coef_[0][i])\n",
    "    #     print(f'{f}')\n",
    "    # inter = '{0:.5f}'.format(clf.intercept_[0])\n",
    "    # print(f'intercept {inter}')\n",
    "    return predicted\n",
    "\n",
    "def fit_and_test(inputs: TestInputs):    \n",
    "    lr_model = LogisticRegression()\n",
    "    rfe_n = rfe_n_features_without_text_tone\n",
    "    if inputs.dataset_features == DatasetFeatures.WITH_TEXT_TONE_AS_COUNTS:\n",
    "        rfe_n = rfe_n_features_with_text_tone_as_count\n",
    "    if inputs.dataset_features == DatasetFeatures.WITH_TEXT_TONE_AS_PERCENTAGES:\n",
    "        rfe_n = rfe_n_features_with_text_tone_as_perentages\n",
    "\n",
    "    # Use RFE to select the top 10 features\n",
    "    rfe = RFE(lr_model, n_features_to_select=rfe_n)\n",
    "    rfe.fit(inputs.x_train, inputs.y_train.iloc[:,0])\n",
    "\n",
    "    # Print the selected features\n",
    "    features_in = [inputs.x_train.columns[i] for i,b in enumerate(rfe.support_) if b == True]\n",
    "    return fit_and_test_sklearn(inputs, features_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20668e5-10ab-4f9c-8802-cb816aee6f77",
   "metadata": {},
   "source": [
    "### The test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a993605c-1c68-4d75-b71e-9ad00f36d875",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "start test Logistic Regression to test 5 Vs All\n",
      "Total records in dataset 747\n",
      "{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "5 Vs All - Without DA features\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.61      0.72        70\n",
      "           1       0.80      0.95      0.86       111\n",
      "\n",
      "    accuracy                           0.82       181\n",
      "   macro avg       0.84      0.78      0.79       181\n",
      "weighted avg       0.83      0.82      0.81       181\n",
      "\n",
      "{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "5 Vs All - With DA features (%)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.69      0.74        70\n",
      "           1       0.82      0.89      0.85       111\n",
      "\n",
      "    accuracy                           0.81       181\n",
      "   macro avg       0.81      0.79      0.80       181\n",
      "weighted avg       0.81      0.81      0.81       181\n",
      "\n",
      "{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "5 Vs All - With DA features (counts)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.59      0.70        70\n",
      "           1       0.78      0.95      0.86       111\n",
      "\n",
      "    accuracy                           0.81       181\n",
      "   macro avg       0.83      0.77      0.78       181\n",
      "weighted avg       0.82      0.81      0.80       181\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "cycle_test('Logistic Regression',fit_and_test,pre_processor=pre_process,test_type=TestType.FIVE_VS_ALL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
